{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/3a/5f7cf24af13265eaea2406d88fc3df5ad870aba0b4f4a0851213519f12bd/tensorflow_gpu-1.1.0-cp36-cp36m-macosx_10_11_x86_64.whl (81.2MB)\n",
      "\u001b[K     |████████████████████████████████| 81.2MB 1.1MB/s eta 0:00:01    |███                             | 7.8MB 816kB/s eta 0:01:30     |██████▍                         | 16.2MB 206kB/s eta 0:05:15\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.0 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from tensorflow-gpu) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from tensorflow-gpu) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from tensorflow-gpu) (0.15.4)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from tensorflow-gpu) (0.33.4)\n",
      "Collecting protobuf>=3.2.0 (from tensorflow-gpu)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/24/d910231174d60c16b3b45db520d21581464049b28ba3562ecd5705c5d5c0/protobuf-3.9.1-cp36-cp36m-macosx_10_9_intel.whl (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 24.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from protobuf>=3.2.0->tensorflow-gpu) (41.0.1)\n",
      "Installing collected packages: protobuf, tensorflow-gpu\n",
      "Successfully installed protobuf-3.9.1 tensorflow-gpu-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==2.0.0-beta1\n",
      "\u001b[31m  ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.0.0-beta1 (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc0, 1.1.0rc1, 1.1.0rc2, 1.1.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow-gpu==2.0.0-beta1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu==2.0.0-beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/8c/7608ba709bd536bc2bccb0d1abbb70aafe9cf7e0170353b4b720ed54cb71/tensorflow-1.14.0-cp36-cp36m-macosx_10_11_x86_64.whl (105.8MB)\n",
      "\u001b[K     |████████████████████████████████| 105.8MB 205kB/s eta 0:00:01    |██████▌                         | 21.5MB 205kB/s eta 0:06:51     |█████████████████████████▉      | 85.5MB 2.1MB/s eta 0:00:10\n",
      "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/a1/cb18aa345c9d5272377f04be871f86b973d9db41937a3c558f4eb16ae924/grpcio-1.23.0-cp36-cp36m-macosx_10_9_x86_64.whl (2.0MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0MB 26.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.11.2)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.16.4)\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.12.0)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from tensorflow) (3.9.1)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/0d/7cbf64cac3f93617a2b6b079c0182e4a83a3e7a8964d3b0cc3d9758ba002/absl-py-0.8.0.tar.gz (102kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 22.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from tensorflow) (0.33.4)\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: h5py in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.15.4)\n",
      "Building wheels for collected packages: gast, absl-py\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/jung-yeongseo/Library/Caches/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/jung-yeongseo/Library/Caches/pip/wheels/9a/1e/7a/456008eb5e47fd5de792c6139df6d5b3d5f71d51c6a0b94799\n",
      "Successfully built gast absl-py\n",
      "Installing collected packages: grpcio, google-pasta, tensorflow-estimator, keras-applications, gast, astor, absl-py, markdown, tensorboard, keras-preprocessing, tensorflow\n",
      "Successfully installed absl-py-0.8.0 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.23.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/6e/0eb0de1c9c4e02df0b40e56f258eb79bd957be79b918511a184268e01720/librosa-0.7.0.tar.gz (1.6MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 307kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting audioread>=2.0.0 (from librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/2e/0b/940ea7861e0e9049f09dcfd72a90c9ae55f697c17c299a323f0148f913d2/audioread-2.1.8.tar.gz\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from librosa) (1.16.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from librosa) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from librosa) (0.21.1)\n",
      "Requirement already satisfied: joblib>=0.12 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from librosa) (0.13.2)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from librosa) (4.4.0)\n",
      "Requirement already satisfied: six>=1.3 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from librosa) (1.12.0)\n",
      "Collecting resampy>=0.2.0 (from librosa)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/75/e22272b9c2185fc8f3af6ce37229708b45e8b855fd4bc38b4d6b040fff65/resampy-0.2.2.tar.gz (323kB)\n",
      "\u001b[K     |████████████████████████████████| 327kB 24.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numba>=0.38.0 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from librosa) (0.45.0)\n",
      "Collecting soundfile>=0.9.0 (from librosa)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/06/58d71f2041bc89919f56a69f8f2b9535a55d513bb005fbe4f8ee5d367170/SoundFile-0.10.2-py2.py3.cp26.cp27.cp32.cp33.cp34.cp35.cp36.pp27.pp32.pp33-none-macosx_10_5_x86_64.macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.whl (616kB)\n",
      "\u001b[K     |████████████████████████████████| 624kB 17.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: llvmlite>=0.29.0dev0 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from numba>=0.38.0->librosa) (0.29.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from soundfile>=0.9.0->librosa) (1.12.3)\n",
      "Requirement already satisfied: pycparser in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa) (2.19)\n",
      "Building wheels for collected packages: librosa, audioread, resampy\n",
      "  Building wheel for librosa (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/jung-yeongseo/Library/Caches/pip/wheels/49/1d/38/c8ad12fcad67569d8e730c3275be5e581bd589558484a0f881\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/jung-yeongseo/Library/Caches/pip/wheels/b9/64/09/0b6417df9d8ba8bc61a7d2553c5cebd714ec169644c88fc012\n",
      "  Building wheel for resampy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/jung-yeongseo/Library/Caches/pip/wheels/fa/c1/56/e0e12c6f7f3d2cdea9712b35136a2d40a7817c6210ec096485\n",
      "Successfully built librosa audioread resampy\n",
      "Installing collected packages: audioread, resampy, soundfile, librosa\n",
      "Successfully installed audioread-2.1.8 librosa-0.7.0 resampy-0.2.2 soundfile-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl (336kB)\n",
      "\u001b[K     |████████████████████████████████| 337kB 304kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from keras) (5.1.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from keras) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from keras) (1.16.4)\n",
      "Requirement already satisfied: h5py in /Users/jung-yeongseo/anaconda3/lib/python3.6/site-packages (from keras) (2.9.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Downloading https://files.pythonhosted.org/packages/79/db/eaf620b73a1eec3c8c6f8f5b0b236a50f9da88ad57802154b7ba7664d0b8/pydub-0.23.1-py2.py3-none-any.whl\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.23.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg\n",
      "  Downloading https://files.pythonhosted.org/packages/f0/cc/3b7408b8ecf7c1d20ad480c3eaed7619857bf1054b690226e906fdf14258/ffmpeg-1.4.tar.gz\n",
      "Building wheels for collected packages: ffmpeg\n",
      "  Building wheel for ffmpeg (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/jung-yeongseo/Library/Caches/pip/wheels/b6/68/c3/a05a35f647ba871e5572b9bbfc0b95fd1c6637a2219f959e7a\n",
      "Successfully built ffmpeg\n",
      "Installing collected packages: ffmpeg\n",
      "Successfully installed ffmpeg-1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffprobe\n",
      "  Downloading https://files.pythonhosted.org/packages/95/9c/adf90d21108d41f611aa921defd2f2e56d3f92724e4b5aa41fae7a9972aa/ffprobe-0.5.zip\n",
      "Building wheels for collected packages: ffprobe\n",
      "  Building wheel for ffprobe (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/jung-yeongseo/Library/Caches/pip/wheels/a0/fe/3b/8af08ae1cdfc27e226779e2cb1a7c8a2ba4954c05c562cdc77\n",
      "Successfully built ffprobe\n",
      "Installing collected packages: ffprobe\n",
      "Successfully installed ffprobe-0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ffprobe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import isfile\n",
    "from pydub import AudioSegment\n",
    "import ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pickle\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, LSTM, Dropout, Activation\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend\n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "from os.path import isfile\n",
    "\n",
    "from timeit import default_timer as timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_names(path=\"/\"):  # class names are subdirectory names in Preproc/ directory\n",
    "    class_names = os.listdir(path)\n",
    "    return class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_files(path=\"/\",train_percentage=0.8): \n",
    "    sum_total = 0\n",
    "    sum_train = 0\n",
    "    sum_test = 0\n",
    "    subdirs = os.listdir(path)\n",
    "#     print(subdirs)\n",
    "    for subdir in subdirs:\n",
    "#         files = os.listdir(path+subdir)\n",
    "        files = path+subdir\n",
    "        n_files = len(files)\n",
    "        sum_total += n_files\n",
    "        n_train = int(train_percentage*n_files)\n",
    "        n_test = n_files - n_train\n",
    "        sum_train += n_train\n",
    "        sum_test += n_test\n",
    "    return sum_total, sum_train, sum_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_dimensions(path='/'):\n",
    "    classname = os.listdir(path)[0]\n",
    "\n",
    "    files =classname\n",
    "    infilename = files[0]\n",
    "\n",
    "    audio_path = path + classname\n",
    "    print(audio_path)\n",
    "    \n",
    "    #여기서 에러난다ㅏㅏㅏ  \n",
    "#     sr = 44100\n",
    "#     melgram, sr = librosa.load(audio_path, sr=sr)\n",
    "#     print(melgram)\n",
    "#     print(type(melgram))\n",
    "    \n",
    "#     melgram = np.load(audio_path)\n",
    "\n",
    "\n",
    "\n",
    "    sr = 16000\n",
    "    y, _ = librosa.load(audio_path, duration=7, sr=sr) # load first seconds\n",
    "\n",
    "    # Calculate RMS\n",
    "    rms_window = 0.1 # in seconds \n",
    "    rms = librosa.feature.rms(y=y, hop_length=int(sr*rms_window))\n",
    "    rms_db = librosa.core.amplitude_to_db(rms, ref=0.0)\n",
    "    print(rms_db)\n",
    "    print(\"   get_sample_dimensions: melgram.shape = \",rms_db.shape)\n",
    "    return rms_db.shape\n",
    " \n",
    "\n",
    "def encode_class(class_name, class_names):  # makes a \"one-hot\" vector for each class name called\n",
    "    try:\n",
    "        idx = class_names.index(class_name)\n",
    "        vec = np.zeros(len(class_names))\n",
    "        vec[idx] = 1\n",
    "        return vec\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def shuffle_XY_paths(X,Y,paths):   # generates a randomized order, keeping X&Y(&paths) together\n",
    "    assert (X.shape[0] == Y.shape[0] )\n",
    "    idx = np.array(range(Y.shape[0]))\n",
    "    np.random.shuffle(idx)\n",
    "    newX = np.copy(X)\n",
    "    newY = np.copy(Y)\n",
    "    newpaths = paths\n",
    "    for i in range(len(idx)):\n",
    "        newX[i] = X[idx[i],:,:]\n",
    "        newY[i] = Y[idx[i],:]\n",
    "        newpaths[i] = paths[idx[i]]\n",
    "    return newX, newY, newpaths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datasets(train_percentage=0.8):\n",
    "#     if (preproc):\n",
    "#         path = \"/\"\n",
    "#     else:\n",
    "#         path = \"Samples/\"\n",
    "    path=\"/Users/jung-yeongseo/Documents/model/data/\"\n",
    "\n",
    "    #파일 이름 뽑아내기\n",
    "    class_names = get_class_names(path=path)\n",
    "    print(\"class_names = \",class_names)\n",
    "    \n",
    "    #파일 길이, \n",
    "    total_files, total_train, total_test = get_total_files(path=path, train_percentage=train_percentage)\n",
    "    print(\"total files = \",total_files)\n",
    "    \n",
    "    \n",
    "    nb_classes = len(class_names)\n",
    "\n",
    "   \n",
    "    # pre-allocate memory for speed (old method used np.concatenate, slow)\n",
    "    mel_dims = get_sample_dimensions(path=path)  # Find out the 'shape' of each data file\n",
    "   \n",
    "#     X_train = np.zeros((total_train, mel_dims[1], mel_dims[2], mel_dims[3]))   \n",
    "    X_train = np.zeros((total_train, mel_dims[0], mel_dims[1]))   \n",
    "    \n",
    "    Y_train = np.zeros((total_train, nb_classes))  \n",
    "#     X_test = np.zeros((total_test, mel_dims[1], mel_dims[2], mel_dims[3]))  \n",
    "    X_test = np.zeros((total_test, mel_dims[0], mel_dims[1]))   \n",
    "    \n",
    "    Y_test = np.zeros((total_test, nb_classes))  \n",
    "    paths_train = []\n",
    "    paths_test = []\n",
    "\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    \n",
    "   \n",
    "    for idx, classname in enumerate(class_names):\n",
    "        this_Y = np.array(encode_class(classname,class_names) )\n",
    "        this_Y = this_Y[np.newaxis,:]\n",
    "    \n",
    "#         class_files = os.listdir(path+classname)\n",
    "        class_files = path+classname\n",
    "        \n",
    "        n_files = len(class_files)\n",
    "        n_load =  n_files\n",
    "        n_train = int(train_percentage * n_load)\n",
    "        printevery = 100\n",
    "        print(\"\")\n",
    "        print(\"n_train=\", n_train)\n",
    "        for idx2, infilename in enumerate(class_files[0:n_load]):          \n",
    "#             audio_path = path + classname + '/' + infilename\n",
    "            audio_path = path + classname\n",
    "            print(\"audio path = \",audio_path)\n",
    "            \n",
    "            if (0 == idx2 % printevery):\n",
    "                print('\\r Loading class: {:14s} ({:2d} of {:2d} classes)'.format(classname,idx+1,nb_classes),\n",
    "                       \", file \",idx2+1,\" of \",n_load,\": \",audio_path,sep=\"\")\n",
    "            #start = timer()\n",
    "#             if (preproc):\n",
    "#               melgram = np.load(audio_path)\n",
    "#               sr = 44100\n",
    "#             else:\n",
    "#               aud, sr = librosa.load(audio_path, mono=mono,sr=None)\n",
    "#               melgram = librosa.logamplitude(librosa.feature.melspectrogram(aud, sr=sr, n_mels=96),ref_power=1.0)[np.newaxis,np.newaxis,:,:]\n",
    "            \n",
    "# \n",
    "#             melgram = melgram[:,:,:,0:mel_dims[1]]   # just in case files are differnt sizes: clip to first file size\n",
    "            melgram = librosa.load(audio_path)\n",
    "            #end = timer()\n",
    "            #print(\"time = \",end - start) \n",
    "            if (idx2 < n_train):\n",
    "                # concatenate is SLOW for big datasets; use pre-allocated instead\n",
    "                #X_train = np.concatenate((X_train, melgram), axis=0)  \n",
    "                #Y_train = np.concatenate((Y_train, this_Y), axis=0)\n",
    "                X_train[train_count,:,:] = melgram\n",
    "                Y_train[train_count,:] = this_Y\n",
    "                paths_train.append(audio_path)     # list-appending is still fast. (??)\n",
    "                train_count += 1\n",
    "            else:\n",
    "                X_test[test_count,:,:] = melgram\n",
    "                Y_test[test_count,:] = this_Y\n",
    "                #X_test = np.concatenate((X_test, melgram), axis=0)\n",
    "                #Y_test = np.concatenate((Y_test, this_Y), axis=0)\n",
    "                paths_test.append(audio_path)\n",
    "                test_count += 1\n",
    "        print(\"\")\n",
    "\n",
    "    print(\"Shuffling order of data...\")\n",
    "    X_train, Y_train, paths_train = shuffle_XY_paths(X_train, Y_train, paths_train)\n",
    "    X_test, Y_test, paths_test = shuffle_XY_paths(X_test, Y_test, paths_test)\n",
    "\n",
    "    return X_train, Y_train, paths_train, X_test, Y_test, paths_test, class_names, sr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_names =  ['오왠_오늘.mp3', 'IU_삐삐.mp3', '.DS_Store', 'Imagine Dragons _Natural.mp3', 'samkim_the one.mp3', '10cm_방에 모기가 있어.mp3']\n",
      "total files =  360\n",
      "/Users/jung-yeongseo/Documents/model/data/오왠_오늘.mp3\n",
      "[[ 4.0298843  4.0298843  4.0298843  4.0298843  4.0298843  4.0298843\n",
      "   4.0298843  4.0298843  4.0298843  4.0298843  4.0298843  4.0298843\n",
      "   4.0298843  4.0298843  4.0298843  4.0298843  4.0298843  4.0298843\n",
      "   4.0298843  4.0298843  4.0298843  4.0298843  4.0298843  4.0298843\n",
      "   4.0298843  4.0298843  4.0298843  4.0298843  4.0298843  4.0298843\n",
      "   4.0298843  4.0298843 55.07615   83.997406  81.7449    80.6207\n",
      "  80.752396  80.82408   80.45572   80.197495  79.82046   81.572624\n",
      "  83.23675   79.926765  79.322495  78.88226   78.68202   78.51262\n",
      "  78.2726    77.903076  77.89718   78.38285   78.03002   77.42971\n",
      "  77.04155   76.33128   75.797775  75.84401   75.60545   79.727295\n",
      "  81.69133   75.91025   75.0146    74.74845   77.853485  79.32312\n",
      "  79.67213   79.58365   81.805466  84.029884  79.56323  ]]\n",
      "   get_sample_dimensions: melgram.shape =  (1, 71)\n",
      "\n",
      "n_train= 45\n",
      "audio path =  /Users/jung-yeongseo/Documents/model/data/오왠_오늘.mp3\n",
      " Loading class: 오왠_오늘.mp3 ( 1 of  6 classes), file 1 of 57: /Users/jung-yeongseo/Documents/model/data/오왠_오늘.mp3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2) into shape (1,71)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-2eb9b0754ae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuild_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-131-f20db77221e6>\u001b[0m in \u001b[0;36mbuild_datasets\u001b[0;34m(train_percentage)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;31m#X_train = np.concatenate((X_train, melgram), axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;31m#Y_train = np.concatenate((Y_train, this_Y), axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmelgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mpaths_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# list-appending is still fast. (??)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2) into shape (1,71)"
     ]
    }
   ],
   "source": [
    "build_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X,Y,nb_classes):\n",
    "    nb_filters = 32  # number of convolutional filters to use\n",
    "    pool_size = (2, 2)  # size of pooling area for max pooling\n",
    "    kernel_size = (3, 3)  # convolution kernel size\n",
    "    nb_layers = 4\n",
    "    input_shape = (1, X.shape[2], X.shape[3])\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid', input_shape=input_shape))\n",
    "    model.add(BatchNormalization(axis=1, mode=2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    for layer in range(nb_layers-1):\n",
    "        model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
    "        model.add(BatchNormalization(axis=1, mode=2))\n",
    "        model.add(ELU(alpha=1.0))  \n",
    "        model.add(MaxPooling2D(pool_size=pool_size))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
